{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Hunter Mitchell**\n\n**Movie Popularity Prediction Project**\n\n**September 22nd, 2020**","metadata":{"id":"jDHHdcHzzjQ3"}},{"cell_type":"markdown","source":"# Intro","metadata":{"id":"60dzQY8p0QKQ"}},{"cell_type":"markdown","source":"***What makes a movie popular?***\n\nThis project attempts to answer that question by examining various factors of films including budget, run time, genre, language and rating!\n\nWe will utilize Machine Learning and Data Science fundamentals to create a model that will predict a movie's popularity. This could be useful for film companies to determine what factors into a movie's popularity, or for other content creators to better understand their audience.\n\nThe data I am using is obtained from a Kaggle dataset found [here](https://www.kaggle.com/tmdb/tmdb-movie-metadata). I encourage anyone to explore the data for themselves and predict other potentially useful imformation (revenue, rating, etc.) \n\nWe must also verify four main assumptions to get valid results from Linear Regression. These are:\n\n1.   Linearity between features and target\n2.   Multivariate normality\n3.   Little multicollinearity\n4.   Homoscedasticity \n\nWe will examine each of these before implementing a Linear Regression model. Now let's get started!","metadata":{"id":"mqEt4GSU0dzZ"}},{"cell_type":"markdown","source":"# Getting to know our data","metadata":{"id":"IXRdfel20wlo"}},{"cell_type":"markdown","source":"Let's begin by importing some necessary libraries so that we can explore our data!","metadata":{"id":"U6NHpdRi3CBj"}},{"cell_type":"code","source":"import pandas as pd # for dataframes\nimport numpy as np # for arrays & math functions\n\n%matplotlib inline\nimport matplotlib.pyplot as plt # for plotting\n\nimport warnings\nwarnings.filterwarnings('ignore') # ignoring any warnings","metadata":{"id":"xCqX2aIBy9jG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PATH = '../input/tmdb-movie-metadata/tmdb_5000_movies.csv'\n\nmovies_df = pd.read_csv(PATH) # load data into a pandas dataframe","metadata":{"id":"8uZVpGzF0zXN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's take a look at a couple instances to see what information we have","metadata":{"id":"-p2vTgmT8MON"}},{"cell_type":"code","source":"SEED = 2020 # for reproducability\n\nmovies_df.sample(3,random_state=SEED)","metadata":{"id":"xHTmqhTx1BKX","outputId":"d637f880-046d-4111-f5fc-fc912db3b9f1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see already that there is some missing data that we will need to take care of. There are also entries of zero that could be false.\n\nAccording to the dataset, popularity is measured as the cumulative number of star ratings. It is also unknown whether the budget and revenue are in USD or some other currency. \n\nLet's look at all of our different columns.\n","metadata":{"id":"qhkz3oUD8dJf"}},{"cell_type":"code","source":"movies_df.info()","metadata":{"id":"9lZ-Yo0_8sEJ","outputId":"71d30e3e-c3c1-46f0-dc69-a05fb36a21d9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As you can see, we have 19 columns to work with. Many of these will not correlate to popularity, though.\n\nOverall, we don't have many null values. There are a lot in the homepage column, but that shouldn't be correlated to popularity, so we can safely ignore it as a feature. Our dataset contains 4,803 instances: not a ton, but it should be plenty for a regression model.\n\nNow let's check out our target variable 'popularity'.","metadata":{"id":"jA43HvxJCdkQ"}},{"cell_type":"code","source":"movies_df['popularity'].describe()","metadata":{"id":"HSEibJ4tduTf","outputId":"88b906d8-665a-4451-e678-123f3c1d613a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It looks like we may have a few outliers present. To see if those are valid instances or not, let's look at the corresponding rows.","metadata":{"id":"KjmxyFihTzho"}},{"cell_type":"code","source":"movies_df.sort_values(by='popularity',ascending=False)[:5]","metadata":{"id":"-SHa-5rcee_u","outputId":"2045ead1-540b-4a0d-b75d-bdfb94606de2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Wow! I rememeber the Minions movie being popular but not *that* popular! The other top movies seem valid as well - I was working at a movie theater when Deadpool came out, and it was massively popular!\n\nAlthough these are outliers, they are still valid instances, and thus we will not modify or drop any of them.\n\nNow let's look at some of the lowest popularities.","metadata":{"id":"zmn_dJrLfHfV"}},{"cell_type":"code","source":"movies_df.sort_values(by='popularity',ascending=True)[:5]","metadata":{"id":"RHTEuhRpi9qz","outputId":"c28248e3-0286-4024-f04d-7e4d3dad73d0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This provides insight into what type of instances we may need to drop - two of these have a runtime of 0! We can also see some missing or suspicious values that we will need to look at. Furthermore, I'm a bit worried about instances when the vote count is very low, as this will make vote average not representative. Let's look at some of the vote average extremes.","metadata":{"id":"lHjw7y6LjIEc"}},{"cell_type":"code","source":"movies_df.sort_values(by='vote_average', ascending = False)[:5]","metadata":{"id":"rK0cc4GgNUV2","outputId":"1d478682-9e0e-4e6d-d258-27840a23a191"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Sure enough, the movies with the best vote averages were only reviewed one or two times! We will have to cut off a certain threshold of vote counts to fix this.","metadata":{"id":"hKnpplMUNv28"}},{"cell_type":"markdown","source":"Now let's take a look at the different languages we have.","metadata":{"id":"d_QZS4juOxxT"}},{"cell_type":"code","source":"movies_df['original_language'].value_counts()","metadata":{"id":"x3olp-FORyn8","outputId":"6718514e-0d50-4f5f-c238-0a34e2c71470"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Pie Chart\n\nlabels = np.array(['English','Other'])\nsizes = np.array([4505, sum(movies_df['original_language'].value_counts()) - 4505])\n\nplt.figure(figsize=(8,9))\n\nplt.pie(sizes, labels=labels, autopct='%1.1f%%', explode=[0,0.08], startangle=90)\nplt.title('Original Languages of Movies', fontdict={'fontsize': 14})\nplt.axis('equal')","metadata":{"id":"kSjBByi7SUVc","outputId":"c3b5e6d0-a172-4b25-969f-c52e0ba567c0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"93.8% of the movies are in english. There are a few different ways we could encode this column. One method would be to one-hot encode by creating a binary column for each different language. This would add a ton of new features to the data, with some only having 1 or 2 positive instances. Another way we could encode this is by converting it to just english or not english. This would only add one column and likely contribute to popularity still.","metadata":{"id":"7yv_4xxZUngP"}},{"cell_type":"markdown","source":"# Cleaning","metadata":{"id":"-hT61IZMIOBK"}},{"cell_type":"markdown","source":"Let's create our features dataframe and clean it up a bit. I am choosing all the features that I believe impact a movies popularity. I am also leaving out revenue as a feature, as this is something revealed a while after a movie comes out.","metadata":{"id":"csyte1oMMlUx"}},{"cell_type":"code","source":"features_df = movies_df[['budget','genres','original_language','runtime','vote_average','vote_count']]\nlabels_df = movies_df['popularity']","metadata":{"id":"ZyHDJ_xKMkHC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"First let's drop any rows with null values","metadata":{"id":"hQFez_ORQXFg"}},{"cell_type":"code","source":"features_df = features_df.dropna()","metadata":{"id":"IdwnP0enISPR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's drop instances with very few vote counts. I decided to have 10 as the cutoff - if 10 or more people voted, it will likely have a fairly accurate vote average. ","metadata":{"id":"dVT1OJNbQYrM"}},{"cell_type":"code","source":"features_df = features_df[features_df['vote_count'] >= 10]","metadata":{"id":"8GOQLIDlIbEl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's drop any values with a runtime of zero","metadata":{"id":"7RcWMDB7QflS"}},{"cell_type":"code","source":"features_df = features_df[features_df['runtime'] != 0.0]","metadata":{"id":"4g1k_efRIdxO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And lastly, we have to set the labels dataframe to only include the rows of the new features dataframe","metadata":{"id":"nkKPPvvAQfH3"}},{"cell_type":"code","source":"labels_df = labels_df[features_df.index]","metadata":{"id":"OhjAcrHZIgGG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's get some insight into our cleaned features","metadata":{"id":"t23sRvUdt2-7"}},{"cell_type":"code","source":"features_df.describe()","metadata":{"id":"PyYfGdEwIh_C","outputId":"b11e4419-138b-4eb8-b6e4-d6f41a245d4c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can already see that the average vote column has no zeros and no 10s, which is good. Furthermore, the shortest runtime is 25 minutes and the longest is 338 minutes, which seem accurate.\n\nThere seem to be some films that have a budget of 0. We will keep these as valid because there are plenty of low-budget films (think Blair Witch Project)","metadata":{"id":"IFAAP_LmIr6I"}},{"cell_type":"markdown","source":"# Exploratory Data Analysis","metadata":{"id":"8gKKVefa-HHn"}},{"cell_type":"markdown","source":"Now it is time to split our data into training and testing data. This is crucial, as we do not want to notice any overall patterns before our models run. We will only use the training data from now on and use our testing data to test our final models.","metadata":{"id":"WbtEqhFePhlY"}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# split data into 80% training and 20% testing\nx_train, x_test, y_train, y_test = train_test_split(features_df, labels_df, test_size=0.2, random_state = SEED)","metadata":{"id":"sG58k65-ofCq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's look at our target variable distribution by plotting a histogram.","metadata":{"id":"ut6h_NGlTXhb"}},{"cell_type":"code","source":"# Plot histogram\nplt.figure(figsize=(10,8))\nplt.hist(y_train.values,bins=50)\nplt.title('Movies popularity histogram')\nplt.xlabel('Popularity')\nplt.ylabel('# of movies')\nplt.show()","metadata":{"id":"kQUGIMKEBHmu","outputId":"88f27d50-d770-4bff-d5d6-f01fd32913ec"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The data is heavily skewed left. The skew method will return how asymmetric the data is (with zero being completely symmetric)","metadata":{"id":"zebb5wjgBVnZ"}},{"cell_type":"code","source":"y_train.skew()","metadata":{"id":"WlfoDlfYCojG","outputId":"b3cf7b22-ad21-43b3-e6ce-ad095b3b2e18"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Linear regression should have all variables be approximately normal, so let's fix this. There are a couple ways we could deal with skewed distributions. These include applying logarithms, square roots, or using the box cox method. I am going to use the log method here. Note that we have to do this to our testing data too. ","metadata":{"id":"GZMzQuFDDHF7"}},{"cell_type":"code","source":"y_train = np.log(y_train)\ny_test = np.log(y_test)","metadata":{"id":"guYRKXwGChXX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot histogram\nplt.figure(figsize=(10,8))\nplt.hist(y_train.values,bins=50)\nplt.title('Movies popularity histogram')\nplt.xlabel('Popularity')\nplt.ylabel('# of movies')\nplt.show()","metadata":{"id":"FrwAILVpCjZU","outputId":"d8734b9f-fe0e-455b-90d1-08128fa6462b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"That looks much better, but let's verify by checking the skew","metadata":{"id":"BPKIKD91CtLQ"}},{"cell_type":"code","source":"print('Popularity skew:', y_train.skew())","metadata":{"id":"rbQk85ODCq--","outputId":"adb3e01b-3d28-4180-b829-804f51a1c42d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Great! Now let's look at the numerical features","metadata":{"id":"xgYNi0Cu421Z"}},{"cell_type":"code","source":"### Plot histograms\nplt.rcParams['figure.figsize'] = 12, 12\nfig, axs = plt.subplots(2,2)\nfig.suptitle('Numerical Feature Histograms',y=0.95,fontsize=16)\n\naxs[0,1].hist(x_train['budget'].values,bins=30,color='salmon')\naxs[0,1].set_title('Budget')\naxs[0,1].set(xlabel='US dollars')\naxs[0,0].hist(x_train['runtime'].values,bins=30,color='salmon')\naxs[0,0].set_title('Runtime (min)')\naxs[0,0].set(xlabel='Minutes')\naxs[1,0].hist(x_train['vote_average'].values,bins=30,color='salmon')\naxs[1,0].set_title('Vote Average')\naxs[1,1].hist(x_train['vote_count'].values,bins=30,color='salmon')\naxs[1,1].set_title('Vote Count')\nplt.show()","metadata":{"id":"erhiqEuh4st3","outputId":"48587022-3304-4c9b-941c-cb4fb68d2fb0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Both runtime and vote average seem fairly normal, but vote count and budget are definitely skewed.","metadata":{"id":"5gElv08D4-wc"}},{"cell_type":"code","source":"print('Vote count skew:', x_train['vote_count'].skew())\nprint('Vote average skew:', x_train['vote_average'].skew())\nprint('Runtime skew:', x_train['runtime'].skew())\nprint('Budget skew:', x_train['budget'].skew())","metadata":{"id":"gmIYLn3STUSl","outputId":"0b7f8766-faa4-43da-af6d-985e7859286a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's normalize the budget and vote count variables. We will do another log transformation to vote count and do a square root transformation to budget. This is because budget contains zero values and taking the log of these would give us undefined values. ","metadata":{"id":"2JQ0TGhyXhi-"}},{"cell_type":"code","source":"x_train['vote_count'] = np.log(x_train['vote_count'].values)\nx_train['budget'] = np.sqrt(x_train['budget'].values)\n\nx_test['vote_count'] = np.log(x_test['vote_count'].values)\nx_test['budget'] = np.sqrt(x_test['budget'].values)","metadata":{"id":"6Zg4HfT5D9qL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's look at the distributions and skew values","metadata":{"id":"GTD_FXcUZva6"}},{"cell_type":"code","source":"### Histograms\n\nplt.rcParams['figure.figsize'] = 12, 12\nfig, axs = plt.subplots(2,2)\nfig.suptitle('Numerical Feature Histograms',y=0.95,fontsize=16)\n\naxs[0,1].hist(x_train['budget'].values,bins=30,color='salmon')\naxs[0,1].set_title('Budget')\naxs[0,1].set(xlabel='US dollars')\naxs[0,0].hist(x_train['runtime'].values,bins=30,color='salmon')\naxs[0,0].set_title('Runtime (min)')\naxs[0,0].set(xlabel='Minutes')\naxs[1,0].hist(x_train['vote_average'].values,bins=30,color='salmon')\naxs[1,0].set_title('Vote Average')\naxs[1,1].hist(x_train['vote_count'].values,bins=30,color='salmon')\naxs[1,1].set_title('Vote Count')\nplt.show()","metadata":{"id":"5-6g5YM7Z1TW","outputId":"88828331-c85e-472c-fb0f-3df22abd874a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(x_train['vote_count'].skew())\nprint(x_train['vote_average'].skew())\nprint(x_train['runtime'].skew())\nprint(x_train['budget'].skew())","metadata":{"id":"2zozLUVKEH6N","outputId":"451620c9-4b90-4f0a-d1df-45b4c3b3e137"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We could work to normalize these more, but for the sake of this project, we will keep it how it is. Now that we have made the distributions more normal, let's look at the correlations between our variables.","metadata":{"id":"YFF4SkiDZ6iV"}},{"cell_type":"code","source":"# Scatterplots\n\nplt.figure(figsize=(12,12))\n\nfig, axs = plt.subplots(2,2)\nfig.suptitle('Correlation to target',y=0.95,fontsize=16)\naxs[0,1].scatter(x_train['vote_average'].values, y_train.values,color='green')\naxs[0,1].set(xlabel='Vote Average',ylabel='Populariy')\naxs[0,0].scatter(x_train['budget'].values, y_train.values,color='green')\naxs[0,0].set(xlabel='Budget (US dollars)',ylabel='Populariy')\naxs[1,0].scatter(x_train['vote_count'].values, y_train.values,color='green')\naxs[1,0].set(xlabel='Vote Count',ylabel='Populariy')\naxs[1,1].scatter(x_train['runtime'].values, y_train.values,color='green')\naxs[1,1].set(xlabel='Runtime (min)',ylabel='Populariy')\nplt.show()","metadata":{"id":"2yh80-HpKugE","outputId":"8dedb452-ee5a-4c7b-9f74-0a17010d3ce1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Vote count definitely looks the most correlated to popularity, but let's check their actual correlation coefficients.","metadata":{"id":"oW_59GPLMCh_"}},{"cell_type":"code","source":"corr_matrix = pd.concat([x_train,y_train],axis=1).corr()\n\ncorr_matrix['popularity'].sort_values(ascending=False)","metadata":{"id":"5LBZ6gquMJP8","outputId":"7d5fdc4f-7923-409d-b89e-1d94e459d3a1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Vote count is indeed the most correlated. The others are still correlated and we can now check off the linearity assumption of Linear Regression. We can also check off our homoscedasticity assumption as the residuals are approximately equal throughout each scatterplot. If they were not equal, we would see a strong cone shape. Now let's look at how our features correlate to each other.","metadata":{"id":"8FbjMavMNGNy"}},{"cell_type":"code","source":"from pandas.plotting import scatter_matrix\n\nscatter_matrix(x_train[['budget','runtime','vote_count','vote_average']], figsize=(12,12))","metadata":{"id":"BAlyyoGcNWYA","outputId":"70af3977-43b8-443b-84ef-dc0afe8b90b1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that there may be a bit of correlation between our independent variables. Let's look at the correlation coefficients.","metadata":{"id":"obbyPsnkOAr2"}},{"cell_type":"code","source":"x_train.corr()","metadata":{"id":"rdeXoMBlcHG5","outputId":"e8bfac5d-628b-4a9e-a6d4-db0cfc69c52b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"While there is a bit more correlation between some independent variables than we would like, we will choose to ignore it for the scope of this project. Therefore, we now have considered all four assumptions and we can move on to getting the data ready for our models!","metadata":{"id":"3KO-HhsfcbAx"}},{"cell_type":"markdown","source":"# Feature Engineering","metadata":{"id":"Cjalhkzt-QAU"}},{"cell_type":"markdown","source":"We still have categorical features that we need to encode. \n\nFor language, we will make a binary variable with 1 for english and 0 for non-english. ","metadata":{"id":"IN2qS3HUQTAH"}},{"cell_type":"code","source":"### Encode language \n\nx_train['Language'] = x_train['original_language'].apply(lambda x: 1 if 'en' == x else 0)\nx_test['Language'] = x_test['original_language'].apply(lambda x: 1 if 'en' == x else 0)","metadata":{"id":"bA27ngi9hgCf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For genres, we will choose to one-hot encode them. Normally, we would use a OneHotEncoder class here, however it will not work with the genre objects we have. This is because every genre entry is a dict with all of the genres they contain, so a OneHotEncoder would make a unique column for each combination. We will have to do it manually.","metadata":{"id":"Y7xaXAJoldOh"}},{"cell_type":"code","source":"genre_list = ['Action', 'Adventure', 'Fantasy', 'Science Fiction', 'Crime', 'Drama', 'Thriller', 'Animation',\n 'Family', 'Western', 'Comedy', 'Romance', 'Horror', 'Mystery', 'History', 'War', 'Music']","metadata":{"id":"RD2T_sVKZc1p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# make column for each genre and encode it\n\nfor genre in genre_list:\n  x_train[genre] = x_train['genres'].apply(lambda x: 1 if genre in x else 0)\n  x_test[genre] = x_test['genres'].apply(lambda x: 1 if genre in x else 0)","metadata":{"id":"vpr2V_yyfWTM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train.describe()","metadata":{"id":"eHXJfP6hgBgv","outputId":"99a97b83-18d6-413c-c903-d244bc4bcbf9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corr_matrix = pd.concat([x_train,y_train],axis=1).corr()\n\ncorr_matrix['popularity'].sort_values(ascending=False)","metadata":{"id":"4pW-1bmrcXUd","outputId":"98d84e1a-33a0-48f1-9390-eca6ab1cea7e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"These results are interesting in themselves! It looks like the adventure and action genres correlate most positively to popularity, and comedy, romance, and drama correlate most negatively. There are a few genres that do not seem to correlate at all, but we will choose to keep them as features.\n\nWe also have to remember to drop our original categorical columns! ","metadata":{"id":"reZGcATUkK2C"}},{"cell_type":"code","source":"x_train = x_train.drop(columns=['original_language','genres']) # drop encoded columns\nx_test = x_test.drop(columns=['original_language','genres']) # drop encoded columns","metadata":{"id":"M6V_hmnSiqpw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's scale our features. Machine Learning models tend to perform better when scaled.","metadata":{"id":"2LjZ-8N-hr_u"}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler, MinMaxScaler\n\nscaler = MinMaxScaler()\n\nx_train_scaled = pd.DataFrame(scaler.fit_transform(x_train),index=x_train.index, columns=x_train.columns)\nx_test_scaled = pd.DataFrame(scaler.transform(x_test),index=x_test.index, columns=x_test.columns)","metadata":{"id":"EpQBUqLHkz6p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Verify\n\nx_train_scaled.head()","metadata":{"id":"m6mYPaxeuTqX","outputId":"5168dfef-a540-4c98-f5a2-872745bd91f5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Perfect! Now we are finally ready to train some models!","metadata":{"id":"Z81vBghG1ZUm"}},{"cell_type":"markdown","source":"# Model Selection","metadata":{"id":"HILlZ5zX-WMH"}},{"cell_type":"markdown","source":"With the data all ready to go, it is finally time to train some models! Let's define a few lists to score our metrics, and a function to print them. The cross_val_score will use cross validation to score the model predictions on all parts of our training data. The main metric we will be looking at is Root Mean Squared Error. This is a common metrics for regression tasks. ","metadata":{"id":"fr6uAYwZ0eMI"}},{"cell_type":"code","source":"from sklearn.model_selection import cross_val_score\n\nrmse_list = []\nstd_list = []\n\ndef get_score(model):\n  cv_score = cross_val_score(model, x_train_scaled, y_train, scoring = \"neg_mean_squared_error\", cv = 8)\n  rmse = np.sqrt(-cv_score)\n  print('Cross-Validation Root Mean Squared Error:', rmse)\n  print('Average Root Mean Squared Error:', round(np.mean(rmse), 5))\n  rmse_list.append(round(np.mean(rmse), 5))\n  print('Standard deviation:', round(rmse.std(), 5))\n  std_list.append(round(rmse.std(), 5))","metadata":{"id":"D_vNgbz20fDS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Regression","metadata":{"id":"OK8qUT7_0Mhs"}},{"cell_type":"markdown","source":"We will begin with the classic linear regression models. Once again, the results and validity of these depend on the four assumptions that we already discussed. \n\n","metadata":{"id":"jl1XImB16PHv"}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression, Ridge, Lasso","metadata":{"id":"2zoQ_1L24Diy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Linear Regression\n\nmodel_1 = LinearRegression()\n\nmodel_1.fit(x_train_scaled,y_train)\n\nget_score(model_1)","metadata":{"id":"ixbANZQ9wVa9","outputId":"fa4b05db-214b-43ee-d0a9-4b6b46262621"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ridge regression adds a regularization term to the cost function which helps prevent overfitting. The term added is the sum of the square of the coefficients.","metadata":{"id":"pM4qC_MMIeAD"}},{"cell_type":"code","source":"### Ridge Regression\n\nmodel_2 = Ridge(random_state=SEED)\n\nmodel_2.fit(x_train_scaled,y_train)\n\nget_score(model_2)","metadata":{"id":"CfJ048tR2Jrm","outputId":"6c49c788-16cc-459e-b3b2-d07399bce726"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lasso regression also adds a regularization term to the cost function, but this time it is the sum of the magnitudes of the coefficients.","metadata":{"id":"z4fA9l5fI8dh"}},{"cell_type":"code","source":"### Lasso Regression\n\nmodel_3 = Lasso(random_state=SEED)\n\nmodel_3.fit(x_train_scaled,y_train)\n\nget_score(model_3)","metadata":{"id":"40LruSWF2SC9","outputId":"8f7d614d-d9ff-45ad-c21a-6e82f3083516"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Random Forest","metadata":{"id":"9BvPOIoL0RYe"}},{"cell_type":"markdown","source":"Random Forest is a simple yet powerful machine learning model. It uses many decision trees to make it's final decision. ","metadata":{"id":"XkN5cigi6Nx4"}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor","metadata":{"id":"iNUvC3rk0T3e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_4 = RandomForestRegressor(random_state=SEED)\n\nmodel_4.fit(x_train_scaled,y_train)\n\nget_score(model_4)","metadata":{"id":"EcNc4N-80XxQ","outputId":"3d5ac9c0-bb5d-4718-b3be-9ffa13853bd1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Support Vector Regressor","metadata":{"id":"1kGuEisT1Rr7"}},{"cell_type":"markdown","source":"Support Vector Machines are typically used for classification tasks, but they can be used for regression as well.","metadata":{"id":"oJ23EXdg6JNo"}},{"cell_type":"code","source":"from sklearn.svm import SVR","metadata":{"id":"ZPIULViL2riH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_5 = SVR()\n\nmodel_5.fit(x_train_scaled,y_train)\n\nget_score(model_5)","metadata":{"id":"5QTNVLM_2pI5","outputId":"9d83b96d-d4ea-4e4b-e1d9-a49eadf5275d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## XGBoost","metadata":{"id":"H7e7wL_Y1UiR"}},{"cell_type":"markdown","source":"XGBoost uses gradient boosted trees to make it's decision. They often outperform most models when the hyperparameters are tuned correctly.","metadata":{"id":"KOJwS1uy6H_7"}},{"cell_type":"code","source":"from xgboost import XGBRegressor","metadata":{"id":"aGNlDGcu28B8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_6 = XGBRegressor(random_state=SEED,verbose=0,objective='reg:squarederror')\n\nmodel_6.fit(x_train_scaled,y_train)\n\nget_score(model_6)","metadata":{"id":"AfA-J2Vm2_6a","outputId":"61a49eeb-e4d1-4f67-9109-72312baa338b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Neural Network","metadata":{"id":"MgnjXp5yz2xD"}},{"cell_type":"markdown","source":"Neural Networks have been around for a while, but have recently taken over all areas of machine learning with architecture advancements and better processing units.  ","metadata":{"id":"WoM0HzSh6K7y"}},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow.keras.layers as L\nfrom keras.wrappers.scikit_learn import KerasRegressor","metadata":{"id":"FhdfNKKA9MeV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I played with the architecture a bit (number of layers, dropout, neurons per layer, etc.) and this seemed to yield the best results.","metadata":{"id":"GfF97q9mItma"}},{"cell_type":"code","source":"def get_tf_model():\n    model = tf.keras.Sequential([\n        L.Input(shape=(x_train_scaled.shape[1])),\n        L.Dense(250, activation='relu'),\n        L.BatchNormalization(),\n        L.Dense(200, activation='relu'),\n        L.BatchNormalization(),\n        L.Dense(200, activation='relu'),\n        L.BatchNormalization(),\n        L.Dense(1)\n    ])\n\n    model.compile(\n        optimizer='adam',\n        loss = 'mse',\n        metrics=['accuracy','mse']\n    )\n    \n    return model","metadata":{"id":"xaTfRHss9aDp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_tf_model().summary()","metadata":{"id":"BwzyLRVK7EIJ","outputId":"c91d8ef7-a343-464c-c197-75f64a46c082"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_7 = KerasRegressor(build_fn = get_tf_model, epochs = 10, verbose = 0, batch_size = 100)\nmodel_7.fit(x_train_scaled,y_train.values)","metadata":{"id":"vR2Rp4xG-FE3","outputId":"2f9463b1-408a-4db4-f212-5e4000030d99"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_score(model_7)","metadata":{"id":"5v4kVxxwxcbR","outputId":"f73e2b48-4788-409b-b5c3-35a41d35162e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Cross-Validation Scores","metadata":{"id":"yptXSd2x8SRi"}},{"cell_type":"markdown","source":"Let's see how the models compare","metadata":{"id":"6grlDHuPOscN"}},{"cell_type":"code","source":"# For creating tables that render in Github\n!pip install --upgrade plotly\n!pip install -U kaleido","metadata":{"id":"rdF7UkSE6vEB","outputId":"797f1a65-c9a0-4189-b4c1-33462f03ab86"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.graph_objects as go\n\n# Create table\n\nmodels_list = ['Linear Regression','Ridge Regression','Lasso Regression','Random Forest', \n               'Support Vector Regressor','XGBoost', 'Neural Network']\n\nfig = go.Figure(data=[go.Table(header=dict(values=['Model', 'RMSE', 'Standard Deviation']),\n                 cells=dict(values=[models_list, rmse_list, std_list]))\n                     ])\n\nfig.update_layout(\n    title={\n        'text': \"Starting Model Cross Validation Scores\",\n        'y':0.9,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'})\n\nfig.show(\"png\")","metadata":{"id":"XLqgJRQvMgpP","outputId":"00679b52-349f-443a-f046-357acf80b964"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Regular Linear Regression and Ridge Regression perform the best. Lasso Regression definitely performs worse - probably because the hyperparameters aren't tuned. XGBoost, Random Forest, and Support Vector Regressor also perform well. Neural Networks kind of let us down, but if we played with the structure more, I'm sure we could make it better.","metadata":{"id":"vviYhp4dKMtw"}},{"cell_type":"markdown","source":"# Hyperparameter Tuning","metadata":{"id":"aa8ObjJo-87k"}},{"cell_type":"markdown","source":"Let's try to improve some of these scores. Kaggle competitors may spend days or even weeks figuring out the best hyperparameters for their models as well as which models to ensemble. \n\n\nWe will just pick a few to improve by using grid search. Grid search reveals which hyperparameter combinations provide the best results by trying many different combinations from what you give it.","metadata":{"id":"tX8XJITx5N99"}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\n# Enter model and parameter options and returns best model\ndef grid_search(model,params):\n  search = GridSearchCV(model, params, cv=5, scoring='neg_mean_squared_error')\n  search.fit(x_train_scaled,y_train)\n  return search.best_estimator_","metadata":{"id":"v5h0PPbl_Vov"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Lasso","metadata":{"id":"vpQs_MsbEZ6E"}},{"cell_type":"markdown","source":"Lasso performed the worst, so let's try to improve it by first looking at what parameters there are.","metadata":{"id":"miN2biWGQOrN"}},{"cell_type":"code","source":"model_3.get_params()","metadata":{"id":"nYV5DFyGFA6A","outputId":"d12b82d9-1f1b-4e01-cc79-d9418f698e85"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We'll pick some options, and apply gird search","metadata":{"id":"QzuDThCGQXs_"}},{"cell_type":"code","source":"param_grid = [\n              {'alpha': [0.1,0.05,0.01,0.005] , \n               \"fit_intercept\": [True, False], \n               'normalize': [True, False],\n               \"tol\": [0.0005,.0001,0.00005]}\n]\n\nmodel_3_grid = grid_search(model_3,param_grid)\n\nmodel_3_grid.get_params() # these will be our new parameters","metadata":{"id":"6fUeZ4dyFjqh","outputId":"a7742d0e-f9da-422a-c955-0b63d18ecc91"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_score(model_3_grid)","metadata":{"id":"xdKqv5R8F8Aj","outputId":"4c2aef25-3e47-42fb-b4ab-3d95741f31e3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Wow! Just like that our RMSE improved a lot!","metadata":{"id":"Jq3K_08UQntu"}},{"cell_type":"markdown","source":"### Support Vector Regressor","metadata":{"id":"FIGc9pZCDsMF"}},{"cell_type":"markdown","source":"Now let's try it for Support Vector Regressor","metadata":{"id":"9nFi9nRzQzik"}},{"cell_type":"code","source":"# Support Vector Regression\n\nmodel_5.get_params()","metadata":{"id":"g0CHKiwLGWmp","outputId":"ca4d9605-34a0-4832-f84d-1b35e8b90f9f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param_grid = [\n              {'kernel': ['linear', 'rbf'],\n               'tol': [0.015, 0.01],\n               'epsilon': [0.2, 0.15] }\n]\n\nmodel_5_grid = grid_search(model_5,param_grid)\n\nmodel_5_grid.get_params()","metadata":{"id":"DVt5a-70GZSy","outputId":"7c299b21-cd8c-40ee-ea89-3032ead24929"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_score(model_5_grid)","metadata":{"id":"P4q8DfVDGdzb","outputId":"c31c8ed3-0356-4bbd-e10b-d6c2df49ae3b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### XGBoost","metadata":{"id":"jZ_BCV7jD2zA"}},{"cell_type":"markdown","source":"And lastly we will use grid search on our XGBoost model","metadata":{"id":"rLMeTFPORH2k"}},{"cell_type":"code","source":"model_6.get_params()","metadata":{"id":"swZ_W5RtKemK","outputId":"614300c0-4bfd-430b-b029-30f29c8c4128"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"param_grid = [\n              {'gamma': [10,5],\n               'max_depth': [7,5],\n               'min_child_weight': [30,20],\n               'learning_rate': [0.05,0.01]}\n]\n\nmodel_6_grid = grid_search(model_6,param_grid)\n\nmodel_6_grid.get_params()","metadata":{"id":"WWY2BAnGKqXv","outputId":"d02248fc-f169-42dc-b806-e10d91a9ade1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_score(model_6_grid)","metadata":{"id":"DRrzkbdyKrQz","outputId":"531a493d-876e-4d04-8e1d-3d60119ddbe2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Comparison","metadata":{"id":"PjJz5m39D-ym"}},{"cell_type":"markdown","source":"We could do this all day to zero in on the absolute best hyperparemters, but for the sake of this project, we will stop here. Let's look at the comparisons","metadata":{"id":"Wc9DXae6McFe"}},{"cell_type":"code","source":"# Create table\n\nmodels_list = ['Lasso Regression','Support Vector Regressor','XGBoost']\n\nfig = go.Figure(data=[go.Table(header=dict(values=['Model', 'Original RMSE', 'Grid Search RMSE', \n                                                   'Original Standard Deviation', 'Grid Search Standard Deviation']),\n                 cells=dict(values=[models_list, [rmse_list[2], rmse_list[4], rmse_list[5]], rmse_list[-3:],\n                                    [std_list[2], std_list[4], std_list[5]], std_list[-3:]]))\n                     ])\n\nfig.update_layout(\n    title={\n        'text': \"Grid Searched Model Comparisons\",\n        'y':0.9,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'})\n\n\nfig.show(\"png\")","metadata":{"id":"_dhjtg9oRlfB","outputId":"fbc83ad0-8bf3-4efe-e2bc-538cdfc7041a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Grid search improved Lasso by a lot, improved Support Vector by a bit, and actually didn't improve XGBoost. This is why it is important to keep trying different combinations of hyperparameter values to see what works. It also improved standard deviation in Lasso and Support Vector, but made XGBoost worse.","metadata":{"id":"Q-ErrPGrMgX-"}},{"cell_type":"markdown","source":"# Results","metadata":{"id":"7uG-tvvx_JP0"}},{"cell_type":"markdown","source":"Up to this point, we have only looked at our training data. Now we can try out our models on the test data to see how they perform on data they have never seen. We will first declare some arrays to store our predictions and scores, and make a function to calculate the Root Mean Squared Error. \n\nIt is also very important to note that the popularity predictions we get will be the logarithm of the actual popularity predictions. Hence, to get the real popularity prediction, we must take the exponential. This is because we need to reverse the log function we applied earlier when we normalized our target variable. ","metadata":{"id":"0rMr2Kij5iWo"}},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\npredictions = []\nfinal_scores = []\n\ndef get_results(preds):\n  score = np.sqrt(mean_squared_error(preds,y_test.values))\n  final_scores.append(round(score,5))","metadata":{"id":"2RSCai-C3SQQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we can make test predictions for each model","metadata":{"id":"xIPDE24nMicr"}},{"cell_type":"code","source":"### Regression\n\npreds = np.array(model_1.predict(x_test_scaled))\npredictions.append(preds)\nget_results(preds)\n\n\n### Ridge\n\npreds = np.array(model_2.predict(x_test_scaled))\npredictions.append(preds)\nget_results(preds)\n\n\n### Lasso\n\npreds = np.array(model_3.predict(x_test_scaled))\npredictions.append(preds)\nget_results(preds)\n\n\n### Forest\n\npreds = np.array(model_4.predict(x_test_scaled))\npredictions.append(preds)\nget_results(preds)\n\n\n### SVR\n\npreds = np.array(model_5.predict(x_test_scaled))\npredictions.append(preds)\nget_results(preds)\n\n\n### XGBoost\n\npreds = np.array(model_6.predict(x_test_scaled))\npredictions.append(preds)\nget_results(preds)\n\n\n### Neural Network\n\npreds = model_7.predict(x_test_scaled).reshape(len(x_test_scaled))\npredictions.append(preds)\nget_results(preds)\n\n\n### Grid searched Lasso\n\npreds = np.array(model_3_grid.predict(x_test_scaled))\npredictions.append(preds)\nget_results(preds)\n\n\n### Grid searched SVR\n\npreds = np.array(model_5_grid.predict(x_test_scaled))\npredictions.append(preds)\nget_results(preds)\n\n\n### Grid searched XGBoost\n\npreds = np.array(model_6_grid.predict(x_test_scaled))\npredictions.append(preds)\nget_results(preds)","metadata":{"id":"zqvVLn6F1ABC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And just for fun, let's try to ensemble some of these predictions and see how they do. The first ensemble will be an average of every model we have made up to this point. The second ensemble will be an average of just the three grid searched models. The third ensemble will be an average of our top three models up to this point (Linear Regression, Ridge Regression, Grid searched Support Vector Regressor). \n","metadata":{"id":"ReQWXliUwfiQ"}},{"cell_type":"code","source":"# average all model predictions\nensemble_1 = np.mean(predictions,axis=0) \nget_results(ensemble_1)\n\n\n# average last three model predictions\nensemble_2 = np.mean(predictions[-3:],axis=0) \nget_results(ensemble_2)\n\n\n# average top three model predictions\nensemble_3 = np.mean([predictions[0], predictions[1],predictions[8]],axis=0) \nget_results(ensemble_3)","metadata":{"id":"bgZWUVOywiXm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's see how the final RMSEs compare between all these models","metadata":{"id":"Zdx2QdqCPaGK"}},{"cell_type":"code","source":"# Create table\n\nmodels_list = ['Linear Regression','Ridge Regression','Lasso Regression','Random Forest','Support Vector Regressor',\n               'XGBoost','Neural Network','Grid Search Lasso','Grid Search Support Vector Regressor',\n               'Grid Search XGBoost','All Model Ensemble','Grid Search Model Ensemble', 'Top 3 Ensemble']\n\nmodels_ranked_df = pd.DataFrame(data={'model': models_list, 'score': final_scores}).sort_values(by='score')\n\nfig = go.Figure(data=[go.Table(header=dict(values=['Model', 'Final RMSE']),\n                 cells=dict(values=[models_ranked_df.model, models_ranked_df.score ]))\n                     ])\n\nfig.update_layout(\n    title={\n        'text': \"All Models Ranked\",\n        'y':0.9,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'})\n\nfig.show(\"png\")","metadata":{"id":"ZQlSCTkJVcEj","outputId":"ca388ff8-fc22-44e3-ecd5-4b83d8dca056"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Surprisingly, our regular XGBoost model performs the best out of all models! We could definitely make it better by finding the right hyperparameters through a more refined grid search. Our grid searched models also perform well, with two of our ensembles coming in 3rd and 4th place. Most of these results are similar to how the models performed on our training data. This is good because it shows there is little to no overfitting and underfitting happening. We could also try some other ensembles to find the best combination. Normally ensembles work the best because they average out all of the residuals between the models. \n\nSpeaking of residuals, let's look at a residuals plot from our best model (XGBoost)","metadata":{"id":"kQvgPrQIXkRP"}},{"cell_type":"code","source":"from yellowbrick.regressor import ResidualsPlot\n\nvisualizer = ResidualsPlot(model_6, is_fitted=True, train_color='b', test_color='g', size=(1080,720))\nvisualizer.fit(x_train_scaled, y_train)\nvisualizer.score(x_test_scaled,y_test)\nvisualizer.poof() ","metadata":{"id":"GBTDAYf2w-In","outputId":"906b7fb1-9bbb-4462-b7f2-665a5d155dfd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This plot gives a lot of useful information. First, our residuals distribution on the right is approximately normal. This is good - if it was skewed, we would not be able to accurately use this information for statistical decisions. Looking at the main section now, we can see that the training and testing residuals are very similar. Our training data seems to be a bit more spread out overall, which we can verify by checking the $R^2$ scores at the top. These measure how well our model fit the data, with 1 being perfect. Since the training score is a bit better than our testing score, there is a bit of overfitting occuring. Ideally, these would about be the same. Our residuals also center around 0 which means that they predict more than the actual value just about as often as they predict less.\n\n","metadata":{"id":"qE8eL2kB3vmV"}},{"cell_type":"markdown","source":"XGBoost also let's us look at how important the features were in it's predictions","metadata":{"id":"JZTa38I2-Ehc"}},{"cell_type":"code","source":"# Create table\n\nfeatures_ranked_df = pd.DataFrame(data={'feature': x_test_scaled.columns, \n                                        'importance': model_6.feature_importances_}\n                                  ).sort_values(by='importance', ascending = False)\n\nfig = go.Figure(data=[go.Table(header=dict(values=['Feature', 'Importance']),\n                 cells=dict(values=[features_ranked_df.feature, [round(x,5) for x in features_ranked_df.importance]]))\n                     ])\n\n\nfig.update_layout(\n    title={\n        'text': \"Features Ranked\",\n        'y':0.9,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'})\n\nfig.show(\"png\")","metadata":{"id":"FgHZ09Ke4G0N","outputId":"e36b7af1-d0bf-4607-f7e6-a0c751978f31"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It looks like vote count was the most important feature. This is expected as it was the most correlated to popularity. We have a somewhat unexpected next most important feature: the War genre. This was one of the least correlated to popularity, so it is definitely surprising to see it so high up on this list. I would have expected our other numerical features to be higher up on this list as well. There are some features not shown as they are approximately of zero importance. This is not too surprising though - there were very few instances with these features to begin with and they showed very little correlation. ","metadata":{"id":"j3hRLOAb6YhZ"}},{"cell_type":"markdown","source":"Any regression model can also provide an $R^2$ score. This is the coefficient of determination and shows how well our model fit the data between -1 and 1. The closer the score is to 1 the better. Let's take a look at how these compare.","metadata":{"id":"mRhuOB98JFGg"}},{"cell_type":"code","source":"def score(model):\n  return round(model.score(x_test_scaled, y_test),5)\n\n# Create table\n\nmodel_r2_list = [score(model_1), score(model_2), score(model_3), score(model_4), score(model_5),\n                 score(model_6), score(model_7), score(model_3_grid), score(model_5_grid), score(model_6_grid)]\n\nr2_ranked_df = pd.DataFrame(data={'model': models_list[:10], 'r2':model_r2_list}\n                                  ).sort_values(by='r2', ascending = False)\n\nfig = go.Figure(data=[go.Table(header=dict(values=['Model', 'R^2']),\n                 cells=dict(values=[r2_ranked_df.model, r2_ranked_df.r2 ]))\n                     ])\n\nfig.update_layout(\n    title={\n        'text': \"R^2 Values\",\n        'y':0.9,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'})\n\nfig.show(\"png\")","metadata":{"id":"m8fP_TY75fu-","outputId":"431162ef-e212-417c-a23e-9d5836e7479b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"These results are not too surprising - they end up very similar to our RMSE table. However, these scores show how much room we still have for improvement. Although 0.84393 is a good score, it still has a bit to go before being a great model. We can also see that our original Lasso Regression model, and our Neural Network fit the data pretty terribly. We were able to improve the Lasso model through grid search, but we will need to experiment quite a bit with Neural Networks to make it more accurate. ","metadata":{"id":"blM6DcH7JN-6"}},{"cell_type":"markdown","source":"# Conclusion","metadata":{"id":"4tciQi5I_KuI"}},{"cell_type":"markdown","source":"From the models that we tried, XGBoost worked the best. I am confident that with more hyperparameter tuning and ensembling, we could achieve a much better RMSE and $R^2$ score. There are also many other models and methods we could try that are outside the scope of this project. I would like to work with on this further in the future with more data. I would also like to predict other things such as revenue or ratings, and try different feature combinations. Overall, this project gave a lot of insight into what increases the popularity of movies. I think it could be useful for production companies or independent filmmakers. I also personally learned a lot doing this and look forward to working on more complex projects in the future! ","metadata":{"id":"iwMUOGh9-IBp"}}]}